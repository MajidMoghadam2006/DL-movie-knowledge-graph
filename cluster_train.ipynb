{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def create_model(hyperParams):\n",
    "    \n",
    "    hidden_layers = hyperParams['hidden_layers']\n",
    "    activation = hyperParams['activation']\n",
    "    dropout = hyperParams['dropout']\n",
    "    output_activation = hyperParams['output_activation']\n",
    "    loss = hyperParams['loss']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_shape=(5000,), activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(len(hidden_layers)-1):\n",
    "        model.add(Dense(hidden_layers[i], activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(46, activation=output_activation))\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy', f1_score])\n",
    "    # categorical_crossentropy, binary_crossentropy f1_loss->(for tensorflow 1.14)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cv_model_fit(X, y, hyperParams):\n",
    "    \n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    scores=[]\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        model = create_model(hyperParams)\n",
    "        model.fit(X[train_idx], y[train_idx], batch_size=hyperParams['batch_size'], \n",
    "                  epochs=hyperParams['epochs'], verbose=0)\n",
    "        score = model.evaluate(X[test_idx], y[test_idx], verbose=0)\n",
    "        scores.append(score[2]*100) # f_score\n",
    "        print('fold ', len(scores), '  score: ', scores[-1])\n",
    "        del model\n",
    "        \n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ID    UTTERANCE    CORE RELATIONS\n",
    "    trainFile = pd.read_csv('dataset/hw1_train.csv')\n",
    "    # ID    UTTERANCE\n",
    "    testFile = pd.read_csv('dataset/hw1_test.csv')\n",
    "\n",
    "    # Data preparation\n",
    "\n",
    "    # input features\n",
    "    X = list(trainFile['UTTERANCE'])\n",
    "\n",
    "    unique_relations = []\n",
    "    for relation_str in trainFile['CORE RELATIONS']:\n",
    "        relations = relation_str.split(' ')\n",
    "        for relation in relations:\n",
    "            if relation not in unique_relations:\n",
    "                unique_relations.append(relation)\n",
    "\n",
    "    unique_nodes = []\n",
    "    for path in unique_relations:\n",
    "        nodes = path.split('.')\n",
    "        for node in nodes:\n",
    "            if node not in unique_nodes:\n",
    "                unique_nodes.append(node)\n",
    "\n",
    "    # add label columns to dataframe\n",
    "    for relation in unique_relations:\n",
    "        trainFile[relation] = 0\n",
    "\n",
    "    # fill out label columns\n",
    "    for idx, relation_str in enumerate(trainFile['CORE RELATIONS']):\n",
    "        relations = relation_str.split(' ')\n",
    "        for relation in relations:\n",
    "            trainFile.loc[idx,relation] = 1\n",
    "\n",
    "    #separate label columns\n",
    "    labels = trainFile[unique_relations]\n",
    "\n",
    "    # target values\n",
    "    y = labels.values\n",
    "\n",
    "    # 5000 words because the training set has almost 2500 unique words\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_matrix(X)\n",
    "    \n",
    "    # hyper parameter optimization\n",
    "    from itertools import product\n",
    "\n",
    "    hyperParams = {'batch_size': 32, 'epochs': 25, 'hidden_layers': [512, 512], 'activation': 'relu', 'dropout': 0.3,\n",
    "                  'output_activation': 'sigmoid', 'loss': 'binary_crossentropy'}\n",
    "\n",
    "    epochs_choices = [15, 25, 40, 100]\n",
    "    activation_choices = ['relu', 'sigmoid', 'tanh']\n",
    "    hidden_layers_choices = [[512, 512], [128, 128, 128], [256, 256, 256], [512, 512, 512],\n",
    "                             [256, 512, 256], [128, 256, 128], [128, 512, 128], [512, 512, 128]]\n",
    "\n",
    "    s = [epochs_choices, activation_choices, hidden_layers_choices]\n",
    "    perms = list(product(*s))  # permutations\n",
    "\n",
    "    best_score = 0\n",
    "    for row in perms:\n",
    "        hyperParams['epochs'] = row[0]\n",
    "        hyperParams['activation'] = row[1]\n",
    "        hyperParams['hidden_layers'] = row[2]\n",
    "        print('10-fold cross validation on these hyperparameters: ', hyperParams, '\\n')\n",
    "        cvscores = cv_model_fit(X, y, hyperParams)\n",
    "        print('\\n-------------------------------------------')\n",
    "        mean_score = np.mean(cvscores)\n",
    "        std_score = np.std(cvscores)\n",
    "        print('CV mean: {0:0.4f},  CV std: {1:0.4f}'.format(mean_score, std_score))\n",
    "        if  mean_score > best_score:    # later I should incorporate std in best model selection\n",
    "            best_score = mean_score\n",
    "            print('****** Best model so far ******')\n",
    "            best_params = hyperParams\n",
    "        print('-------------------------------------------\\n')\n",
    "    \n",
    "    # build last model from full data\n",
    "    model = create_model(best_params)\n",
    "    model.fit(X, y, batch_size=best_params['batch_size'], epochs=best_params['epochs'], verbose=1)\n",
    "    \n",
    "    # test file\n",
    "    X_t = list(testFile['UTTERANCE'])\n",
    "    X_pred = tokenizer.texts_to_matrix(X_t)\n",
    "    \n",
    "    # generate predictions\n",
    "    y_pred = model.predict(X_pred)\n",
    "    \n",
    "    # tailor and prepare predictions\n",
    "    predictions = []\n",
    "    for y in y_pred:\n",
    "        temp = []\n",
    "        for i,v in enumerate(y):\n",
    "            if v >= 0.5:\n",
    "                temp.append(unique_relations[i])\n",
    "        if len(temp) < 1:   # select max if no prob >= 0.5\n",
    "            temp.append(unique_relations[np.argmax(y)])\n",
    "        if (len(temp) > 1) and ('other' in temp):   # remove 'other' if more than 1 labels\n",
    "            temp.remove('other')\n",
    "        if (len(temp) > 1) and ('NO_REL' in temp):   # remove 'NO_REL' if more than 1 labels\n",
    "            temp.remove('NO_REL')\n",
    "        if (len(temp) > 1) and ('movie_other' in temp):   # remove 'movie_other' if more than 1 labels\n",
    "            temp.remove('movie_other')\n",
    "        predictions.append(' '.join(temp))\n",
    "    \n",
    "    # generate submission file\n",
    "    submissionFile = pd.DataFrame({\n",
    "        'ID': [i for i in range(len(predictions))],\n",
    "        'CORE RELATIONS': predictions})\n",
    "    path_to_save = os.path.abspath(os.getcwd()) + '/predictions/'\n",
    "    submissionFile.to_csv('predictions/prediction17.csv', index = None)\n",
    "    submissionFile.head()\n",
    "    print('FINISHED!')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
